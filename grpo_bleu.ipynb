{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "请严格按照以下格式回答:\n",
    "[ASSISTANT]\n",
    "<回答内容>\n",
    "[/ASSISTANT]\n",
    "\"\"\"\n",
    "import json\n",
    "def get_from_local(json_path: str) -> Dataset:\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict({\n",
    "        'question': [item['question'] for item in data],  # 添加原始问题\n",
    "        'prompt': [\n",
    "            [\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': item['question']}\n",
    "            ]\n",
    "            for item in data\n",
    "        ],\n",
    "        'answer': [item['answer'] for item in data]\n",
    "    })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=get_from_local(\"grpo_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '[HUMAN1] 我今日感覺幾好，期待活動開始！[HUMAN2] 好開心見到咁多朋友，希望今日可以學到新嘢。',\n",
       " 'prompt': [{'content': '\\n请严格按照以下格式回答:\\n[ASSISTANT]\\n<回答内容>\\n[/ASSISTANT]\\n',\n",
       "   'role': 'system'},\n",
       "  {'content': '[HUMAN1] 我今日感覺幾好，期待活動開始！[HUMAN2] 好開心見到咁多朋友，希望今日可以學到新嘢。',\n",
       "   'role': 'user'}],\n",
       " 'answer': '[ASSISTANT] 好開心見到各位，準備好開始未？'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has the correct [ASSISTANT]...[/ASSISTANT] format.\"\"\"\n",
    "    pattern = r\"^\\[ASSISTANT\\](.*?)\\[/ASSISTANT\\]$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_assistant_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion contains [ASSISTANT]...[/ASSISTANT] tags.\"\"\"\n",
    "    pattern = r\"\\[ASSISTANT\\](.*?)\\[/ASSISTANT\\]\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.search(pattern, r, re.DOTALL) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_assistant_format(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.startswith(\"[ASSISTANT]\"):\n",
    "        count += 0.25\n",
    "    else:\n",
    "        if \"[ASSISTANT]\" in text:\n",
    "            count += 0.125\n",
    "    if text.endswith(\"[/ASSISTANT]\"):\n",
    "        count += 0.25\n",
    "    else:\n",
    "        if \"[/ASSISTANT]\" in text:\n",
    "            count += 0.125\n",
    "    if text.endswith(\"[/ASSISTANT]\"):\n",
    "        prefix = text.split(\"[ASSISTANT]\")[0]\n",
    "        count -= len(prefix.strip()) * 0.001\n",
    "        suffix = text.split(\"[/ASSISTANT]\")[-1]\n",
    "        count -= len(suffix.strip()) * 0.001\n",
    "    start_tags = text.count(\"[ASSISTANT]\")\n",
    "    end_tags = text.count(\"[/ASSISTANT]\")\n",
    "    if start_tags > 1:\n",
    "        count -= (start_tags - 1) * 0.1\n",
    "    if end_tags > 1:\n",
    "        count -= (end_tags - 1) * 0.1\n",
    "        \n",
    "    return max(0.0, count)  \n",
    "\n",
    "def assistant_format_count_reward(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks the [ASSISTANT]...[/ASSISTANT] format and provides granular scoring.\"\"\"\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_assistant_format(c) for c in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def get_ngrams(segment, n):\n",
    "    \"\"\"\n",
    "    获取文本中的n元组\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    for i in range(len(segment) - n + 1):\n",
    "        ngram = segment[i:i + n]\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "def count_ngrams(segment, n):\n",
    "    \"\"\"\n",
    "    计算n元组的出现次数\n",
    "    \"\"\"\n",
    "    ngram_counts = Counter()\n",
    "    for ngram in get_ngrams(segment, n):\n",
    "        ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "def modified_precision(candidate, reference, n):\n",
    "    \"\"\"\n",
    "    计算修正的n元组精确度\n",
    "    \"\"\"\n",
    "    candidate_ngrams = count_ngrams(candidate, n)\n",
    "    reference_ngrams = count_ngrams(reference, n)\n",
    "    \n",
    "    if len(candidate_ngrams) == 0:\n",
    "        return 0\n",
    "    \n",
    "    clipped_counts = {}\n",
    "    for ngram, count in candidate_ngrams.items():\n",
    "        clipped_counts[ngram] = min(count, reference_ngrams[ngram])\n",
    "    \n",
    "    numerator = sum(clipped_counts.values())\n",
    "    denominator = sum(candidate_ngrams.values())\n",
    "    \n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def brevity_penalty(candidate, reference):\n",
    "    \"\"\"\n",
    "    计算简短惩罚因子\n",
    "    \"\"\"\n",
    "    c = len(candidate)\n",
    "    r = len(reference)\n",
    "    \n",
    "    if c > r:\n",
    "        return 1\n",
    "    elif c == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.exp(1 - r/c)\n",
    "\n",
    "def calculate_bleu4(candidate, reference):\n",
    "    \"\"\"\n",
    "    计算BLEU-4得分\n",
    "    \"\"\"\n",
    "    # 计算1-4元组的精确度\n",
    "    p1 = modified_precision(candidate, reference, 1)\n",
    "    p2 = modified_precision(candidate, reference, 2)\n",
    "    p3 = modified_precision(candidate, reference, 3)\n",
    "    p4 = modified_precision(candidate, reference, 4)\n",
    "    \n",
    "    # 如果有任何n元组精确度为0，整体BLEU分数为0\n",
    "    if 0 in (p1, p2, p3, p4):\n",
    "        return 0\n",
    "    \n",
    "    # 计算几何平均\n",
    "    avg = math.exp((math.log(p1) + math.log(p2) + math.log(p3) + math.log(p4)) / 4)\n",
    "    \n",
    "    # 计算简短惩罚因子\n",
    "    bp = brevity_penalty(candidate, reference)\n",
    "    \n",
    "    return bp * avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_reward_func(prompts, completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    计算BLEU奖励分数\n",
    "    Args:\n",
    "        prompts: 输入提示\n",
    "        completions: 模型生成的完成\n",
    "    Returns:\n",
    "        float: 奖励分数\n",
    "    \"\"\"\n",
    "    # 确保我们正确获取responses和query\n",
    "    responses = []\n",
    "    for completion in completions:\n",
    "        if isinstance(completion, list):\n",
    "            response = completion[0]['content'].replace(\"[ASSISTANT]\",'')\n",
    "        else:\n",
    "            response = completion['content'].replace(\"[ASSISTANT]\",'')\n",
    "        if isinstance(response, list):\n",
    "            response = ' '.join(response)  # 如果response是list，将其连接成字符串\n",
    "        responses.append(response)\n",
    "    \n",
    "    # 获取查询文本\n",
    "    query = prompts[0][-1]['content']\n",
    "    if isinstance(query, list):\n",
    "        query = ' '.join(query)  # 如果query是list，将其连接成字符串\n",
    "    \n",
    "    # 初始化奖励列表\n",
    "    rewards = []\n",
    "    \n",
    "    # 为每个response计算BLEU分数和奖励\n",
    "    for response in responses:\n",
    "        try:\n",
    "            bleu_score = calculate_bleu4(response, query)\n",
    "            \n",
    "            # 根据BLEU分数确定奖励\n",
    "            if bleu_score < 0.4:\n",
    "                reward = 0.0\n",
    "            elif bleu_score == 1.0:\n",
    "                reward = 2.0\n",
    "            else:\n",
    "                reward = 1.0 + ((bleu_score - 0.4) / 0.6)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating BLEU score: {e}\")\n",
    "            rewards.append(0.0)  # 发生错误时返回0分\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 19:53:13 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n",
    "from vllm import LLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512 # Can increase for longer reasoning traces\n",
    "lora_rank = 16 # Larger rank = smarter, but slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/home/yyf/data/hub/Qwen2.5-1.5B-Instruct\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,model_max_length=max_seq_length)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 19:53:14,678] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyf/anaconda3/envs/grpo/compiler_compat/ld: cannot find -laio: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyf/anaconda3/envs/grpo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/yyf/anaconda3/envs/grpo/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    # optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    bf16 = True,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 8, # Decrease if out of memory\n",
    "    max_prompt_length = 256,\n",
    "    max_completion_length = 200,\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    # max_steps = 250,\n",
    "    save_steps = 500,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs/GRPO_model\",\n",
    "    vllm_device=\"cuda:0\",\n",
    "    vllm_gpu_memory_utilization=0.3,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        assistant_format_count_reward,\n",
    "        assistant_format_reward_func,\n",
    "        soft_assistant_format_reward_func,\n",
    "        bleu_reward_func,\n",
    "        \n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.00037098174119201135, metrics={'train_runtime': 14.2875, 'train_samples_per_second': 0.7, 'train_steps_per_second': 0.7, 'total_flos': 0.0, 'train_loss': 0.00037098174119201135})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
